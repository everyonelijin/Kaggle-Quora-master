{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3_3.5\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "import os\n",
    "# to reload files that are changed automatically\n",
    "# import utils file - contains some useful functions\n",
    "import quora_dup_utils as qu\n",
    "questions_folder_name = \"cleaned_data\"\n",
    "google_model_path = '/home/daniel/Desktop/word2vec/word2vec-C-code/data/GoogleNews-vectors-negative300.bin' # path to saved Google word2vec model\n",
    "\n",
    "# file to store doc2vec hyperparameters and ROC AUC errors\n",
    "parameters_and_errors_name = \"parameters_and_errors.csv\"\n",
    "\n",
    "# number of question pairs to use in training doc2vec\n",
    "# total number of pairs is currently 404288\n",
    "num_question_pairs = 404288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404290 non-null object\n",
      "question2       404290 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "data = pd.read_csv('C:/Users/lWX453896/Desktop/python/Quora Question Pairs/train.csv')\n",
    "data = data.fillna('empty')\n",
    "data.info()\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=True):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "def process_questions(question_list, questions, question_list_name, dataframe):\n",
    "    '''transform questions and display progress'''\n",
    "    for question in questions:\n",
    "        question_list.append(text_to_wordlist(question))\n",
    "        if len(question_list) % 100000 == 0:\n",
    "            progress = len(question_list)/len(dataframe) * 100\n",
    "            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"question1\"] = data.question1.str.lower()\n",
    "data[\"question2\"] = data.question2.str.lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_question1 is 24.7% complete.\n",
      "train_question1 is 49.5% complete.\n",
      "train_question1 is 74.2% complete.\n",
      "train_question1 is 98.9% complete.\n"
     ]
    }
   ],
   "source": [
    "train_question1 = []\n",
    "process_questions(train_question1, data.question1, 'train_question1', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_question2 is 24.7% complete.\n",
      "train_question2 is 49.5% complete.\n",
      "train_question2 is 74.2% complete.\n",
      "train_question2 is 98.9% complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_question2 = []\n",
    "process_questions(train_question2, data.question2, 'train_question2', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step by step guid invest in share market',\n",
       " 'would happen indian govern stole kohinoor koh i noor diamond back']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_question2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404290 non-null object\n",
      "question2       404290 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/lWX453896/Desktop/python/Quora Question Pairs/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=   pd.concat([data.id,pd.DataFrame(train_question1),pd.DataFrame(train_question2),data.is_duplicate],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns=['id','question1','question2', 'is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step by step guid invest in share market in india',\n",
       " 'stori kohinoor koh i noor diamond',\n",
       " 'how can i increas speed my internet connect use vpn',\n",
       " 'whi am i mental veri lone how can i solv it',\n",
       " 'one dissolv in water quick sugar salt methan carbon di oxid']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.question1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_data = train_question1 +train_question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step by step guid invest in share market in india',\n",
       " 'stori kohinoor koh i noor diamond',\n",
       " 'how can i increas speed my internet connect use vpn']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self,question_num, filename):\n",
    "        self.question_num = question_num\n",
    "        self.filename = filename \n",
    "    def __iter__(self):\n",
    "        for uid, text_line in enumerate(self.filename):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(words=text_line.split(), tags=[self.question_num,uid])\n",
    "\n",
    "# make sure using fast version\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "cores = multiprocessing.cpu_count() # number of cores on computer to use for computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "sentences = LabeledLineSentence('question1',list(data.question1))\n",
    "for sentence in sentences:\n",
    "    all_docs.append(sentence)\n",
    "sentences = LabeledLineSentence('question2',list(data.question2))\n",
    "for sentence in sentences:\n",
    "    all_docs.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808580"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['gastro', 'pub'], tags=['question2', 195710]),\n",
       " LabeledSentence(words=['happen', 'in', 'last', 'episod', 'detect', 'conan', 'case', 'close', 'how', 'doe', 'it', 'end'], tags=['question2', 195711]),\n",
       " LabeledSentence(words=['it', 'like', 'be', 'summer', 'student', 'at', 'yale', 'how', 'it', 'differ', 'from', 'normal', 'school', 'year'], tags=['question2', 195712]),\n",
       " LabeledSentence(words=['how', 'can', 'you', 'improv', 'your', 'communic', 'skill'], tags=['question2', 195713]),\n",
       " LabeledSentence(words=['happen', 'when', 'debt', 'ceil', 'higher', 'interest', 'on', 'america', 'debt'], tags=['question2', 195714]),\n",
       " LabeledSentence(words=['who', 'invent', 'blow', 'dryer', 'how', 'was', 'it', 'invent'], tags=['question2', 195715]),\n",
       " LabeledSentence(words=['how', 'should', 'one', 'chang', 'their', 'diet', 'lose', 'weight'], tags=['question2', 195716]),\n",
       " LabeledSentence(words=['sinc', 'airbag', 'add', 'safeti', 'whi', 'don', 't', 'profession', 'race', 'car', 'like', 'in', 'f1', 'nascar', 'wrc', 'have', 'them'], tags=['question2', 195717]),\n",
       " LabeledSentence(words=['are', 'caus', 'nois', 'pollut'], tags=['question2', 195718]),\n",
       " LabeledSentence(words=['how', 'do', 'messag', 'app', 'whatsapp', 'make', 'money'], tags=['question2', 195719])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[600000:600010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404290"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data.is_duplicate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set model parameters parameters\n",
    "\n",
    "parameters_dict = {\n",
    "\n",
    "'documents' : all_docs,\n",
    "'dm' : 0, # use bag-of-words (dbow) model; 1 uses embedding (dmpv) model\n",
    "'size' : 200, # size of word/doc vectors\n",
    "'window' : 15, # # max distance between word and neighbor word for word embeddings\n",
    "'alpha' : .025, # learning rate - use rate in paper\n",
    "'min_alpha' : 0.0001, # rate from paper\n",
    "'min_count' : 5, # ignore words with count less than this\n",
    "'sample' : 1e-5, # how to configure downsampling for high frequency words\n",
    "'workers' : cores, # number of cores to use\n",
    "'hs' : 0, # use negative sampling\n",
    "'negative' : 5, # used in negative sampling\n",
    "'dbow_words' : 1, # trains word vectors in addition to document vectors in dbow model\n",
    "'iter' : 3 # recommended number of epochs is ~20 for dbow model on question comparison   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create list of parameters to use in model\n",
    "from itertools import product\n",
    "dms = [0]\n",
    "sizes = [300]\n",
    "windows = [5]\n",
    "alphas = [0.025]\n",
    "min_alphas = [0.0001]\n",
    "min_counts = [1, 5]\n",
    "samples = [1e-5]\n",
    "workers_s = [cores]\n",
    "hs_s = [0]\n",
    "negatives = [5]\n",
    "dbow_words_s = [1]\n",
    "iters = [150]\n",
    "\n",
    "# create iterable of all combinations of parameters\n",
    "params_product = product(dms, sizes, windows, alphas, min_alphas, \n",
    "                        min_counts, samples, workers_s, hs_s, negatives, \n",
    "                        dbow_words_s, iters)\n",
    "parameters = [x for x in params_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_and_errors = []\n",
    "\n",
    "print \"Starting first run of\", len(parameters), \"runs\"\n",
    "total_time = 0\n",
    "for run_number, pars in enumerate(parameters): \n",
    "    params = {'dm':pars[0], 'size':pars[1], 'window':pars[2], \n",
    "              'alpha':pars[3], 'min_alpha':pars[4], 'min_count':pars[5],\n",
    "              'sample':pars[6], 'workers':pars[7], 'hs':pars[8],\n",
    "              'negative':pars[9], 'dbow_words':pars[10], 'iter':pars[11]}\n",
    "    with qu.elapsed_timer() as elapsed:\n",
    "        model = gensim.models.doc2vec.Doc2Vec(documents=all_docs, **params)\n",
    "                                                                                                            AUC_value = qu.calculate_AUC(model, doc_names_and_duplicate_class)\n",
    "        duration = '%.1f' % elapsed()\n",
    "        # save time to complete computation\n",
    "        m, s = divmod(float(duration), 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        time_string = \"%dh %02dm %02ds\" % (h, m, s)\n",
    "        params_and_errors.append((params, AUC_value, time_string))\n",
    "        total_time += float(duration)\n",
    "        print \n",
    "        print \"Completed run number\", run_number + 1, \"of\", len(parameters), \"runs total\"\n",
    "        print \"AUC score:\", round(AUC_value, 4)\n",
    "        print \"Training for this run took\", round(float(duration)/60.,1), \"minutes\"\n",
    "\n",
    "best_AUC = max([x[1] for x in params_and_errors])\n",
    "print\n",
    "print\n",
    "print \"Total training time for all runs:\", round(float(total_time)/3600.,2), \"hours\"\n",
    "print \"Best AUC value:\", round(best_AUC, 6)\n",
    "print \"Paramters for best AUC value:\", [x[0] for x in params_and_errors if x[1] == best_AUC][0]\n",
    "\n",
    "# convert params and errors into easy-to-read pandas dataframe\n",
    "params_df = pd.DataFrame([x[0] for x in params_and_errors])\n",
    "params_df[\"AUC\"] = pd.Series([x[1] for x in params_and_errors])\n",
    "params_df[\"num_doc_pairs\"] = pd.Series([len(all_docs) for _ in range(len(params_and_errors))])\n",
    "params_df[\"compute_time\"] = pd.Series([x[2] for x in params_and_errors])\n",
    "params_df.sort_values(\"AUC\", ascending=False, inplace=True)\n",
    "\n",
    "# write parameter values to csv - append if this csv already exists\n",
    "header=True\n",
    "if os.path.isfile(parameters_and_errors_name):\n",
    "    header=False\n",
    "params_df.to_csv(parameters_and_errors_name, header=header, index=False, mode=\"a\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
